{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> \n",
    "    CPT_S 434/534 Neural network design and application\n",
    "</center></h1>\n",
    "\n",
    "<!-- <h2><center>CPT_S 434/534</center></h2> -->\n",
    "\n",
    "<h2><center>HW 1: ML -- code (80 pts)</center></h2>\n",
    "\n",
    "### Name: *[SAIMAN DAHAL]*\n",
    "\n",
    "### Student ID: *[11873444]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This assignment includes:\n",
    "\n",
    "## Coding in Python (pytorch): train a regularized Softmax classifier on MNIST \n",
    "\n",
    "### Step 0: Install and configure: python ([Anaconda platform](https://docs.anaconda.com/anaconda/install/) recommended), [Jupyter Notebook](https://jupyter.org/install), [pytorch](https://pytorch.org/get-started/), [torchvision](https://anaconda.org/pytorch/torchvision), etc.\n",
    "\n",
    "**Remark 1.** [Colab](https://colab.research.google.com) is a cloud platform that enables your Jupyter Notebooks (including this .ipynb assignment) to run with different runtime types (hardware acceleration is possible using GPU or TPU). You may also choose Colab to finish assignments (future assignments may require extensive computation that may be time-consuming on your laptop such as 1 or 2 hours). \n",
    "\n",
    "**Remark 2.** If you use Colab, make sure the working path/directory is the current one. See [this page](https://stackoverflow.com/questions/48298146/changing-directory-in-google-colab-breaking-out-of-the-python-interpreter).\n",
    "\n",
    "**Remark 3.** If you use Colab, it is still required to convert your .ipynb to .html and submit **BOTH** files to Canvas. See [this page](https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab) on how to convert to .html using Colab\n",
    "\n",
    "### Step 1: Read the [provided document](CPTS434_534_HW1_note.pdf) to understand how to train a regularized Softmax classifier:\n",
    "\n",
    "**Remark 1.** What is the problem setting of the multiclass classification\n",
    "\n",
    "**Remark 2.** What is Cross-Entropy loss and $\\color{red}{\\text{the objective function of the regularized Softmax classifier}}$\n",
    "\n",
    "**Remark 3.** How to compute the gradients of the objective function w.r.t. the weight matrix/vectors\n",
    "\n",
    "**Remark 4.** What is the general algorithmic process of mini-batch SGD\n",
    "\n",
    "### Step 2: Read provided code (with pytorch) to understand the logic of design and target, so that you know how to implement in the following step and incorporate your code into the provided code\n",
    "\n",
    "### Step 3: Complete the code\n",
    "\n",
    "#### 1) \"mini_batch_gradient()\" (**36 pts**) for computing mini-batch stochastic gradients in MB-SGD, and train the linear model with setting 1\n",
    "\n",
    "#### 2) write the configure files for other 6 new settings and train their classifiers (4 pts each, **24 pts** in total). \n",
    "\n",
    "### Step 4: Record and plot results to show the loss and accuracy convergence (against #epoch)\n",
    "\n",
    "### Step 5: Answer the five questions at the end of this file (4 pts each, **20 pts** in total)\n",
    "\n",
    "## $\\color{red}{\\text{Submission:}}$\n",
    "\n",
    "* Convert the .ipynb file to .html file (**save the execution outputs** to show your progress: otherwise grading may be affected)\n",
    "    \n",
    "* Upload **both** your .ipynb and .html files to Canvas separately (not zip them to a single zip file).\n",
    "\n",
    "* Plots should be clear and easy to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. (Read and run) Load/Download MNIST dataset using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (60000, 784)\n",
      "Shape of x_test: (10000, 784)\n",
      "Shape of y_train: (60000,)\n",
      "Shape of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_set = datasets.MNIST('./data', train=True, download=True)\n",
    "test_set = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "x_train = train_set.data.numpy()\n",
    "x_train = x_train.reshape(len(x_train),-1)\n",
    "x_test = test_set.data.numpy()\n",
    "x_test = x_test.reshape(len(x_test),-1)\n",
    "\n",
    "y_train = train_set.targets.numpy()\n",
    "y_test = test_set.targets.numpy()\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. (Read and run) Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. (Read and run) Visualize a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its label is\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "d = x_train.shape[0]\n",
    "random_index = numpy.random.randint(d, size=1)[0]\n",
    "plt.imshow(x_train[random_index].reshape(28,28))\n",
    "print('Its label is')\n",
    "print(y_train[random_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Read and run) Define softmax function and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"implement the softmax functions\n",
    "    input: numpy ndarray\n",
    "    output: numpy ndarray\n",
    "    \"\"\"\n",
    "    exp_list = numpy.exp(z)\n",
    "    result = 1/sum(exp_list) * exp_list\n",
    "    result = result.reshape((len(z),1))\n",
    "    assert (result.shape == (len(z),1))\n",
    "    return result\n",
    "\n",
    "def neg_log_loss(pred, label):\n",
    "    \"\"\"implement the negative log loss\"\"\"\n",
    "    loss = -numpy.log(pred[int(label)])\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (Read and run) Functions for implementing linear softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "root = os.path.abspath('.')\n",
    "root += '/configs/'\n",
    "\n",
    "def loadConfig(name):\n",
    "    \"\"\" Read a configuration file as a dictionary\"\"\"\n",
    "    full_path = root + name\n",
    "    json_file = open(full_path, 'r')\n",
    "    cfg = json.load(json_file)\n",
    "    json_file.close()\n",
    "    return cfg  \n",
    "\n",
    "def initialize(num_inputs,num_classes):\n",
    "    \"\"\"initialize the parameters\"\"\"\n",
    "    # num_inputs = 28*28 = 784\n",
    "    # num_classes = 10\n",
    "    w = numpy.zeros((num_classes, num_inputs)) # (10*784)\n",
    "    \n",
    "    param = {\n",
    "        'w' : w, # (10*784)\n",
    "    }\n",
    "    return param\n",
    "\n",
    "def eval(param, hyp, x_data, y_data):\n",
    "    \"\"\" implement the evaluation function\n",
    "    input: param -- parameters dictionary (w)\n",
    "           hyp -- hyper-parameter: we use hyp['lambda'] to compute regularization\n",
    "           x_data -- x_train or x_test (size, 784)\n",
    "           y_data -- y_train or y_test (size,)\n",
    "    output: loss and accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    reg_lambda = hyp['lambda']\n",
    "    \n",
    "    # w: (10*784), x: (10000*784), y:(10000,)\n",
    "    loss_list = []\n",
    "    w = param['w'].transpose()\n",
    "    dist = numpy.array([numpy.squeeze(softmax(numpy.matmul(x_data[i], w))) for i in range(len(y_data))])\n",
    "\n",
    "    result = numpy.argmax(dist,axis=1)\n",
    "    accuracy = sum(result == y_data)/float(len(y_data))\n",
    "\n",
    "    loss_list = [neg_log_loss(dist[i],y_data[i]) for i in range(len(y_data))]\n",
    "    loss = sum(loss_list) / len(loss_list) + reg_lambda/2 * numpy.sum(w * w)\n",
    "    return loss, accuracy\n",
    "\n",
    "def train(param, hyp, x_train, y_train, x_test, y_test,cfg_idx):\n",
    "    \"\"\" implement the train function\n",
    "    input: param -- parameters dictionary (w)\n",
    "           hyp -- hyperparameters dictionary\n",
    "           x_train -- (60000, 784)\n",
    "           y_train -- (60000,)\n",
    "           x_test -- x_test (10000, 784)\n",
    "           y_test -- y_test (10000,)\n",
    "    output: train_loss_list, train_acc_list, test_loss_list, test_acc_list\n",
    "           Four lists contain the epoch-wise loss function on training data, accuracy on training data, loss function on testing data, accuracy on testing data, respectively\n",
    "    \"\"\"\n",
    "    num_epoches = hyp['num_epoches']\n",
    "    batch_size = hyp['batch_size']\n",
    "    learning_rate = hyp['learning_rate']\n",
    "    mu = hyp['mu']\n",
    "    reg_lambda = hyp['lambda']\n",
    "    train_loss_list, train_acc_list, test_loss_list, test_acc_list = [],[],[],[]\n",
    "    if bool(hyp['momentum']) == True:\n",
    "        w_velocity = numpy.zeros(param['w'].shape)\n",
    "        b_velocity = numpy.zeros(param['b'].shape) \n",
    "\n",
    "    for epoch in range(num_epoches):\n",
    "        \n",
    "        # select the random sequence of training set\n",
    "        rand_indices = numpy.random.choice(x_train.shape[0],x_train.shape[0],replace=False)\n",
    "        num_batch = int(x_train.shape[0]/batch_size)\n",
    "        \n",
    "        if bool(hyp['learning_decay']) == True:\n",
    "            try:\n",
    "                if test_acc_list[-1] - test_acc_list[-2] < 0.001:\n",
    "                    learning_rate *= hyp['decay_factor']\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            message = 'learning rate: %.8f' % learning_rate\n",
    "            print(message)\n",
    "            logging.info(message)\n",
    "\n",
    "        # for each batch of train data\n",
    "        for batch in range(num_batch):\n",
    "            index = rand_indices[batch_size*batch:batch_size*(batch+1)]\n",
    "            x_batch = x_train[index]\n",
    "            y_batch = y_train[index]\n",
    "\n",
    "            # calculate the stochastic gradient w.r.t w \n",
    "            dw, batch_loss = mini_batch_gradient(param, x_batch, y_batch, reg_lambda)\n",
    "\n",
    "            param['w'] -= learning_rate * dw\n",
    "            \n",
    "            if (batch+1) % 100 == 0:\n",
    "                message = 'Epoch [%d/%d], Batch [%d/%d], Loss %.4f' % (epoch+1, num_epoches, batch+1, num_batch, batch_loss)\n",
    "                print(message)\n",
    "\n",
    "        train_loss, train_acc = eval(param,hyp,x_train,y_train)\n",
    "        test_loss, test_acc = eval(param,hyp,x_test,y_test)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        message = 'Epoch %d/%d, Train Loss %.4f, Train Acc %.4f, Test Loss %.4f, Test Acc %.4f' % (epoch+1, num_epoches, train_loss, train_acc, test_loss, test_acc)\n",
    "        print(message)\n",
    "        logging.info(message)\n",
    "    return train_loss_list, train_acc_list, test_loss_list, test_acc_list\n",
    "\n",
    "\n",
    "def plot(train_loss_list, train_acc_list, test_loss_list, test_acc_list, cfg_idx):\n",
    "    \"\"\"store the plots\"\"\"\n",
    "    # epoch_list = list(range(len(loss_list)))\n",
    "    plt.plot(train_loss_list, '-b', label='train loss')\n",
    "    plt.plot(test_loss_list, '-r', label='test loss')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Loss Function')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title('Loss Function ~ Epoch')\n",
    "    plt.savefig('assets/loss_{}.png'.format(cfg_idx))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(train_acc_list, '-b', label='train acc')\n",
    "    plt.plot(test_acc_list, '-r', label='test acc')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title('Accuracy ~ Epoch')\n",
    "    plt.savefig('assets/accr_{}.png'.format(cfg_idx))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def main(cfg_idx): \n",
    "#     cfg_idx = args.config\n",
    "    cfg_name = 'config_{}.json'.format(cfg_idx)\n",
    "    hyperpara = loadConfig(cfg_name)\n",
    "\n",
    "    # setting the random seed\n",
    "    numpy.random.seed(1024)\n",
    "\n",
    "    # initialize the parameters\n",
    "    num_inputs = x_train.shape[1]\n",
    "    num_classes = len(set(y_train))\n",
    "    param = initialize(num_inputs,num_classes)\n",
    "\n",
    "    # train the model\n",
    "    train_loss_list, train_acc_list, test_loss_list, test_acc_list = train(param,hyperpara,x_train,y_train,x_test,y_test, cfg_idx)\n",
    "\n",
    "    # plot the loss and accuracy\n",
    "    plot(train_loss_list, train_acc_list, test_loss_list, test_acc_list, cfg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. (To finish, 36 pts) Implementation of mini-batch gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient(param, x_batch, y_batch, reg_lambda):\n",
    "    \"\"\"Compute the mini-batch gradient.\n",
    "\n",
    "    Args:\n",
    "        param (dict): Parameters dictionary containing 'w'.\n",
    "        x_batch (numpy.ndarray): Input data for the mini-batch.\n",
    "        y_batch (numpy.ndarray): Labels for the mini-batch.\n",
    "        reg_lambda (float): Regularization parameter.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Gradient w.r.t. weight 'w'.\n",
    "        float: Average loss on the mini-batch samples.\n",
    "    \"\"\"\n",
    "    weight = param['w']\n",
    "    distance=numpy.array([numpy.squeeze(softmax(numpy.matmul(x_batch[i],weight.T))) for i in range(len(y_batch))])\n",
    "    list_=[neg_log_loss(distance[i],y_batch[i]) for i in range (len(y_batch))]\n",
    "    \n",
    "    loss=sum(list_)\n",
    "    avg=loss/len(x_batch)\n",
    "\n",
    "    # \n",
    "    regularization=(reg_lambda/2)*sum(sum(numpy.matmul(weight,weight.T)))\n",
    "    batch_loss=loss+regularization\n",
    "\n",
    "    # \n",
    "    d_batch=[]\n",
    "    for data_index, label_weights in enumerate(dist):\n",
    "        d_i=[]\n",
    "        for class_index, weight_ in enumerate(label_weights):\n",
    "            if (y_batch[data_index]==class_index):\n",
    "                d_i.append(numpy.matmul((weight_-1), x_batch[data_index]))\n",
    "            else:\n",
    "                d_i.append(numpy.matmul((weight_-0), x_batch[data_index]))\n",
    "\n",
    "        d_batch.append(numpy.array(d_i))\n",
    "\n",
    "\n",
    "    dw_=[]\n",
    "    for weight_ in weight:\n",
    "        dw_.append(weight=reg_lambda)\n",
    "\n",
    "    dw=sum(numpy.array(d_batch))/length(x_batch) + numpy.array(dw_)\n",
    "\n",
    "    \n",
    "\n",
    "    return dw, batch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Read and run) Train your model using the provided configuration \"setting 1\" (in *configs/config_setting_1.json*) untill convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massets\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('logs') == False:\n",
    "    os.mkdir('logs')\n",
    "    \n",
    "if os.path.isdir('assets') == False:\n",
    "    os.mkdir('assets')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cfg_idx = 'setting_1'\n",
    "\n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"./logs/{}.log\".format(cfg_idx), filemode=\"w\", format=\"%(message)s\", level=logging.DEBUG)\n",
    "    \n",
    "    main(cfg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (To finish, 4 pts) Create a new configuration file for setting 2 (see comment below) and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cfg_idx = 'setting_2'\n",
    "\n",
    "# configuration in setting 2:\n",
    "#     {\n",
    "#     \"num_epoches\" : 50,\n",
    "#     \"batch_size\" : 600,\n",
    "#     \"learning_rate\" : 0.1,\n",
    "#     \"learning_decay\" : 0,\n",
    "#     \"decay_factor\" : 0.75,\n",
    "#     \"momentum\" : 0,\n",
    "#     \"lambda\" : 0.00,\n",
    "#     \"mu\" : 0.9\n",
    "# }\n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"./logs/{}.log\".format(cfg_idx), filemode=\"w\", format=\"%(message)s\", level=logging.DEBUG)\n",
    "    \n",
    "    main(cfg_idx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. (To finish, 4 pts) Create a new configuration file for setting 3 (see comment below) and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cfg_idx = 'setting_3'\n",
    "    \n",
    "# configuration in setting 3:\n",
    "#     {\n",
    "#     \"num_epoches\" : 25,\n",
    "#     \"batch_size\" : 100,\n",
    "#     \"learning_rate\" : 0.1,\n",
    "#     \"learning_decay\" : 0,\n",
    "#     \"decay_factor\" : 0.75,\n",
    "#     \"momentum\" : 0,\n",
    "#     \"lambda\" : 0.00,\n",
    "#     \"mu\" : 0.9\n",
    "# }\n",
    "    \n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"./logs/{}.log\".format(cfg_idx), filemode=\"w\", format=\"%(message)s\", level=logging.DEBUG)\n",
    "    \n",
    "    main(cfg_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. (To finish, 4 pts) Create a new configuration file for setting 4 (see comment below) and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cfg_idx = 'setting_4'\n",
    "    \n",
    "# configuration in setting 4:\n",
    "# {\n",
    "#     \"num_epoches\" : 25,\n",
    "#     \"batch_size\" : 100,\n",
    "#     \"learning_rate\" : 0.01,\n",
    "#     \"learning_decay\" : 0,\n",
    "#     \"decay_factor\" : 0.75,\n",
    "#     \"momentum\" : 0,\n",
    "#     \"lambda\" : 0.00,\n",
    "#     \"mu\" : 0.9\n",
    "# }\n",
    "    \n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"./logs/{}.log\".format(cfg_idx), filemode=\"w\", format=\"%(message)s\", level=logging.DEBUG)\n",
    "    \n",
    "    main(cfg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. (To finish, 4 pts) Create a new configuration file for setting 5 (see comment below) and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cfg_idx = 'setting_5'\n",
    "    \n",
    "# configuration in setting 5:\n",
    "#     {\n",
    "#     \"num_epoches\" : 50,\n",
    "#     \"batch_size\" : 600,\n",
    "#     \"learning_rate\" : 0.1,\n",
    "#     \"learning_decay\" : 0,\n",
    "#     \"decay_factor\" : 0.75,\n",
    "#     \"momentum\" : 0,\n",
    "#     \"lambda\" : 0.001,\n",
    "#     \"mu\" : 0.9\n",
    "# }\n",
    "    \n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"./logs/{}.log\".format(cfg_idx), filemode=\"w\", format=\"%(message)s\", level=logging.DEBUG)\n",
    "    \n",
    "    main(cfg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. (To finish, 4 pts) Create a new configuration file for setting 6 (see comment below) and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cfg_idx = 'setting_6'\n",
    "    \n",
    "# configuration in setting 6:\n",
    "#     {\n",
    "#     \"num_epoches\" : 50,\n",
    "#     \"batch_size\" : 600,\n",
    "#     \"learning_rate\" : 0.1,\n",
    "#     \"learning_decay\" : 0,\n",
    "#     \"decay_factor\" : 0.75,\n",
    "#     \"momentum\" : 0,\n",
    "#     \"lambda\" : 0.01,\n",
    "#     \"mu\" : 0.9\n",
    "# }\n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"./logs/{}.log\".format(cfg_idx), filemode=\"w\", format=\"%(message)s\", level=logging.DEBUG)\n",
    "    \n",
    "    main(cfg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. (To finish, 3 pts) Create a new configuration file for setting 7 (see below) and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cfg_idx = 'setting_7'\n",
    "    \n",
    "# configuration in setting 7:\n",
    "#     {\n",
    "#     \"num_epoches\" : 50,\n",
    "#     \"batch_size\" : 600,\n",
    "#     \"learning_rate\" : 0.1,\n",
    "#     \"learning_decay\" : 0,\n",
    "#     \"decay_factor\" : 0.75,\n",
    "#     \"momentum\" : 0,\n",
    "#     \"lambda\" : 0.1,\n",
    "#     \"mu\" : 0.9\n",
    "# }\n",
    "    \n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"./logs/{}.log\".format(cfg_idx), filemode=\"w\", format=\"%(message)s\", level=logging.DEBUG)\n",
    "    \n",
    "    main(cfg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. (To finish, 4 pts each question, 20 pts in total) Answer the following five questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 (4 pts): What can we learn by comparing results of setting 1 and setting 2? Please analyse your observation/conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (4 pts): What can we learn by comparing results of setting 2 and setting 3? Please analyse your observation/conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (4 pts): What can we learn by comparing results of setting 3 and setting 4? Please analyse your observation/conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 (4 pts): What can we learn by comparing results of setting 2 and 5 (or 6)? Please analyse your observation/conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 (4 pts): What may happen to setting 7, compared to settings 2, 5 and 6?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook HW1_to_Canvas.ipynb to html\n",
      "[NbConvertApp] Writing 656244 bytes to HW1_to_Canvas.html\n"
     ]
    }
   ],
   "source": [
    "# convert this file \"HW1_to_Canvas.ipynb\" to \"HW1_to_Canvas.html\"\n",
    "!jupyter nbconvert --to html HW1_to_Canvas.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
